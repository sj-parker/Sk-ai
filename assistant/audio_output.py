import re
import json
import asyncio
import uuid
import os
import edge_tts
import requests
import subprocess
from overlay_server import send_to_overlay, send_text_to_overlay, send_emotion_to_vrm, send_animation_to_vrm, send_status_to_vrm, send_speech_to_vrm, send_text_to_vrm, send_to_vrm
from monitor import error_log, last_answers
import time
import inspect
from collections import OrderedDict
from config import get, get_tts_engine
from typing import Optional
import threading
from vrm_websocket import init_vrm_server, send_mouth_open
import monitor

# –ò–º–ø–æ—Ä—Ç –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º —Å–µ—Ä–≤–µ—Ä–∞
try:
    from audio_stream_server import audio_server
    AUDIO_STREAM_AVAILABLE = True
    print("[AUDIO STREAM] –ú–æ–¥—É–ª—å –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º–∞ –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError as e:
    AUDIO_STREAM_AVAILABLE = False
    print(f"[AUDIO STREAM] –ú–æ–¥—É–ª—å –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

# --- Silero TTS ---
import torch
import soundfile as sf
import numpy as np

VOICE = "ru-RU-SvetlanaNeural"
TMP_FOLDER = "temp_audio"
os.makedirs(TMP_FOLDER, exist_ok=True)

# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏–º—ã–µ –∫—É—Å–∫–∏
punctuation_pattern = re.compile(r'[.!?‚Ä¶\n]+')

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–π LLM-–∫—ç—à ---
class LLMCache(OrderedDict):
    def __init__(self, maxsize=1000):
        super().__init__()
        self.maxsize = maxsize
    def __getitem__(self, key):
        value = super().__getitem__(key)
        self.move_to_end(key)
        return value
    def __setitem__(self, key, value):
        if key in self:
            self.move_to_end(key)
        super().__setitem__(key, value)
        if len(self) > self.maxsize:
            oldest = next(iter(self))
            del self[oldest]
llm_cache = LLMCache(maxsize=1000)

def silero_tts(text, filename, speaker='baya', sample_rate=48000):
    model, _ = torch.hub.load(
        repo_or_dir='snakers4/silero-models',
        model='silero_tts',
        language='ru',
        speaker='v3_1_ru'
    )
    audio = model.apply_tts(text=text, speaker=speaker, sample_rate=sample_rate)
    sf.write(filename, audio, sample_rate)

async def stream_lipsync_from_wav(filename, duration=None, frame_ms=40):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç wav/mp3-—Ñ–∞–π–ª, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç energy lipsync –≤ VRM overlay.
    frame_ms ‚Äî –¥–ª–∏–Ω–∞ –æ–∫–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö (–æ–±—ã—á–Ω–æ 30-50–º—Å)
    duration ‚Äî –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ (—Å–µ–∫), –µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–∞
    """
    try:
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª (mp3/wav)
        data, samplerate = sf.read(filename)
        if len(data.shape) > 1:
            data = data.mean(axis=1)  # –º–æ–Ω–æ
        frame_len = int(samplerate * frame_ms / 1000)
        total_frames = len(data) // frame_len
        for i in range(total_frames):
            frame = data[i*frame_len:(i+1)*frame_len]
            if len(frame) == 0:
                continue
            rms = np.sqrt(np.mean(frame**2))
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º energy –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0..1 (—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏)
            energy = min(1.0, float(rms) * 5)
            await send_to_vrm({"type": "lipsync", "energy": energy})
            await asyncio.sleep(frame_ms / 1000)
        # –í –∫–æ–Ω—Ü–µ ‚Äî —Ä–æ—Ç –∑–∞–∫—Ä—ã—Ç—å
        await send_to_vrm({"type": "lipsync", "energy": 0.0})
    except Exception as e:
        print(f"[LIPSYNC ERROR]: {e}")

async def speak_text(text: str):
    tts_engine = get_tts_engine()
    ext = '.wav'  # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ wav –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    filename = os.path.join(TMP_FOLDER, f"{uuid.uuid4().hex}{ext}")
    try:
        await init_vrm_server()
        if tts_engine == 'silero':
            silero_tts(text, filename)
            lipsync_task = asyncio.create_task(stream_lipsync_from_wav(filename))
        else:
            # EdgeTTS: –ø—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é –≤ wav
            communicate = edge_tts.Communicate(text, VOICE)
            await communicate.save(filename)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ —Ñ–∞–π–ª wav (–∏–Ω–æ–≥–¥–∞ EdgeTTS —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç mp3 —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º wav)
            try:
                _ = sf.info(filename)
                wav_for_lipsync = filename
            except Exception:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ wav ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º mp3 –≤ wav
                mp3_filename = filename.replace('.wav', '.mp3')
                os.rename(filename, mp3_filename)
                wav_for_lipsync = filename
                subprocess.run(['ffmpeg', '-y', '-i', mp3_filename, wav_for_lipsync], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            lipsync_task = asyncio.create_task(stream_lipsync_from_wav(wav_for_lipsync))
        
        await send_mouth_open(0.7)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ –≤ —Å—Ç—Ä–∏–º (–µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω)
        if AUDIO_STREAM_AVAILABLE:
            try:
                # –ß–∏—Ç–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ —Å—Ç—Ä–∏–º
                with open(filename, 'rb') as f:
                    audio_data = f.read()
                await audio_server.broadcast_audio(audio_data)
                print(f"[AUDIO STREAM] –ê—É–¥–∏–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ —Å—Ç—Ä–∏–º ({len(audio_data)} –±–∞–π—Ç)")
            except Exception as e:
                print(f"[AUDIO STREAM] –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ: {e}")
        else:
            print("[AUDIO STREAM] –ê—É–¥–∏–æ —Å—Ç—Ä–∏–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –ü—Ä–æ–∏–≥—Ä—ã–≤–∞–µ–º –∞—É–¥–∏–æ –ª–æ–∫–∞–ª—å–Ω–æ (ffplay)
        proc = subprocess.Popen(
            ['ffplay', '-nodisp', '-autoexit', '-loglevel', 'quiet', filename],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏—è –∏ lipsync
        await asyncio.to_thread(proc.wait)
        await lipsync_task
        await send_mouth_open(0.0)
    finally:
        if os.path.exists(filename):
            os.remove(filename)
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π mp3, –µ—Å–ª–∏ –±—ã–ª
        mp3_filename = filename.replace('.wav', '.mp3')
        if os.path.exists(mp3_filename):
            os.remove(mp3_filename)

# --- –í—ã–Ω–µ—Å—Ç–∏ emoji_map –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –º–æ–¥—É–ª—è ---
emoji_map = {
    'üòä': 'happy',
    'üòÑ': 'happy',
    'üò¢': 'sad',
    'üò≠': 'sad',
    'üò°': 'angry',
    'üò±': 'surprised',
    'üòâ': 'happy',
    'üòÉ': 'happy',
    'üòÜ': 'happy',
    'üòÇ': 'happy',
    'üòê': 'neutral',
    'üòî': 'sad',
    'üòû': 'sad',
    'üòÖ': 'happy',
    'üòè': 'happy',
    'üòú': 'happy',
    'üòã': 'happy',
    'üò§': 'angry',
    'üò¨': 'surprised',
    'üò≥': 'surprised',
    'üòá': 'happy',
    'üòé': 'happy',
    'ü•≤': 'sad',
    'ü•π': 'sad',
    'ü•∞': 'happy',
    'üòç': 'happy',
    'üòò': 'happy',
    'üòö': 'happy',
    'üòô': 'happy',
    'üòó': 'happy',
    'üòö': 'happy',
    'üòΩ': 'happy',
    'üò∫': 'happy',
    'üò∏': 'happy',
    'üòπ': 'happy',
    'üòª': 'happy',
    'üòº': 'happy',
    'üòΩ': 'happy',
    'üòæ': 'angry',
    'üòø': 'sad',
    'üôÄ': 'surprised',
}

def extract_emotion(text):
    for emoji, emotion in emoji_map.items():
        if emoji in text:
            return emotion
    return None

# --- –î–û–ë–ê–í–õ–ï–ù–û: emoji -> animation ---
emoji_to_animation = {
    'üò±': 'surprise_move',
    'üò≥': 'surprise_move',
    'üôÄ': 'surprise_move',
    'ü§Ø': 'surprise_move',
    'ü´¢': 'surprise_move',
    'ü•∂': 'surprise_move',
    'ü•¥': 'surprise_move',
    'ü´®': 'surprise_move',
    'üòÆ': 'surprise_move',
    'üò≤': 'surprise_move',
    'üòØ': 'surprise_move',
    'üòµ': 'surprise_move',
    'üòµ\u200düí´': 'surprise_move',  # üòµ‚Äçüí´
    'ü•±': 'surprise_move',
    'ü§≠': 'surprise_move',
    'ü§î': 'surprise_move',
    'üò°': 'activeTalking_move',
    'üò§': 'activeTalking_move',
    'üòä': 'greeting_move',
    'üòÑ': 'greeting_move',
    'ü•∞': 'greeting_move',
    'üò¢': 'thinking_move',
    'üò≠': 'thinking_move',
    # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å –ø–æ –∂–µ–ª–∞–Ω–∏—é
    # --- –í–æ–ª–Ω–µ–Ω–∏–µ (excitement) ---
    'ü§©': 'excitement_move',
    'ü§ó': 'excitement_move',
    'ü•≥': 'excitement_move',
    'ü§™': 'excitement_move',
    'ü§†': 'excitement_move',
    'ü§ì': 'excitement_move',
    'ü§ò': 'excitement_move',
    '‚ú®': 'excitement_move',
    'üíÉ': 'excitement_move',
    'üï∫': 'excitement_move',
    'ü•Ç': 'excitement_move',
    'üéâ': 'excitement_move',
    'üéä': 'excitement_move',
    # --- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ (greeting) ---
    'üëã': 'greeting_move',
    'üñêÔ∏è': 'greeting_move',
    '‚úã': 'greeting_move',
    'ü§ö': 'greeting_move',
    'ü´±': 'greeting_move',
    'ü´≤': 'greeting_move',
    'ü´≥': 'greeting_move',
    'ü´¥': 'greeting_move',
    'ü§ù': 'greeting_move',
    'üôè': 'greeting_move',
    'üôå': 'greeting_move',
    'üëè': 'greeting_move',
}

def clean_text_for_tts(text):
    filters = get('TTS_FILTERS', {})
    # –£–¥–∞–ª–∏—Ç—å –≤—Å—ë, —á—Ç–æ –º–µ–∂–¥—É *–∑–≤—ë–∑–¥–æ—á–∫–∞–º–∏*
    if filters.get('remove_star_text', True):
        text = re.sub(r'\*.*?\*', '', text)
    # –£–¥–∞–ª–∏—Ç—å emoji
    if filters.get('remove_emoji', True):
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ emoji (unicode –¥–∏–∞–ø–∞–∑–æ–Ω—ã)
        emoji_pattern = re.compile("[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]+", flags=re.UNICODE)
        text = emoji_pattern.sub('', text)
    # –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    allowed = filters.get('allowed_chars', 'a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9 ')
    text = re.sub(f'[^{allowed}]', '', text)
    return text

# --- –î–û–ë–ê–í–ò–¢–¨: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —ç–º–æ—Ü–∏–π –∏ –¥–≤–∏–∂–µ–Ω–∏–π ---
def extract_emotion_and_animation(text):
    found_emotion = None
    found_animation = None
    for char in text:
        if not found_emotion and char in emoji_map:
            found_emotion = emoji_map[char]
        if not found_animation and char in emoji_to_animation:
            found_animation = emoji_to_animation[char]
        if found_emotion and found_animation:
            break
    return found_emotion, found_animation

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –∏ –¥–≤–∏–∂–µ–Ω–∏–π ---
last_emotion = None
last_animation = None
emotion_reset_time = 0
animation_reset_time = 0

# model phi3:mini or gemma3:4b or gemma:2b or gemma3n:e2b
async def chat_stream_ollama(role_manager, memory, prompt, model_name="gemma3n:e2b", speak=True, user_name=None):
    steps = []
    step_start = time.time()
    cache_key = (role_manager.get_role(), prompt)
    if cache_key in llm_cache:
        full_response = llm_cache[cache_key]
        print("[LLM CACHE] –û—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞.")
        await send_text_to_overlay(full_response.strip())
        last_answers.append({'source': 'local', 'user': 'user', 'text': full_response.strip(), 'timestamp': time.strftime('%H:%M:%S')})
        return full_response.strip()

    memory.add("user", prompt)
    steps.append({"name": "add_to_short_term", "duration": time.time() - step_start}); step_start = time.time()

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ –ø–∞–º—è—Ç–∏ ---
    memory_response = memory.handle_memory_commands(prompt)
    steps.append({"name": "memory_commands", "duration": time.time() - step_start}); step_start = time.time()
    if memory_response:
        print(f"[MEMORY] {memory_response}")
        await send_text_to_overlay(memory_response)
        last_answers.append({'source': 'local', 'user': 'user', 'text': memory_response, 'timestamp': time.strftime('%H:%M:%S')})
        monitor.timing_history.append({"steps": steps, "total": sum(s["duration"] for s in steps)})
        if len(monitor.timing_history) > 5:
            monitor.timing_history.pop(0)
        return memory_response

    # --- –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–º–µ—à–∞–Ω–Ω–æ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç—å—é ---
    context_prompt = memory.get_context_as_system_prompt(user_name)
    steps.append({"name": "get_context_as_system_prompt", "duration": time.time() - step_start}); step_start = time.time()
    context_messages = [context_prompt] if context_prompt else []

    # --- –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ) ---
    recall_keywords = ["–≤—Å–ø–æ–º–Ω–∏", "–ø–æ–º–Ω–∏—à—å"]
    recall_hits = []
    if any(kw in prompt.lower() for kw in recall_keywords):
        # –ü–æ–∏—Å–∫ –≤ long_term
        recall_hits.extend(memory.search_memory(prompt, persistent=False, max_results=3))
        # –ü–æ–∏—Å–∫ –≤ persistent (–µ—Å–ª–∏ –µ—Å—Ç—å)
        recall_hits.extend(memory.search_memory(prompt, persistent=True, max_results=2))
    steps.append({"name": "search_memory", "duration": time.time() - step_start}); step_start = time.time()

    # üí° –í—Å—Ç–∞–≤–ª—è–µ–º —Ä–æ–ª—å + –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç—å—é + —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –ø–∞–º—è—Ç—å + –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é
    messages = (
        [{"role": "system", "content": role_manager.get_role()}]
        + context_messages
        + recall_hits
        + memory.get_short_memory()
    )
    try:
        import monitor
        monitor.last_llm_prompt = messages
    except Exception:
        pass
    steps.append({"name": "form_messages", "duration": time.time() - step_start}); step_start = time.time()

    url = "http://localhost:11434/api/chat"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": model_name,
        "messages": messages,
        "stream": True
    }

    buffer = ""
    full_response = ""
    last_text_update = ""

    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "thinking" –≤ VRM
        await send_status_to_vrm("thinking")
        steps.append({"name": "overlay_thinking", "duration": time.time() - step_start}); step_start = time.time()
        
        with requests.post(url, json=data, headers=headers, stream=True) as response:
            steps.append({"name": "llm_request_start", "duration": time.time() - step_start}); step_start = time.time()
            for line in response.iter_lines():
                if not line:
                    continue
                line = line.decode('utf-8').strip()
                if line.startswith("data:"):
                    line = line[5:].strip()

                try:
                    json_data = json.loads(line)
                    content = json_data.get("message", {}).get("content", "")
                    if content:
                        print(content, end="", flush=True)
                        buffer += content
                        full_response += content

                        # --- –≠–º–æ—Ü–∏–∏ —á–µ—Ä–µ–∑ overlay –∏ VRM ---
                        now = time.time()
                        emotion, animation = extract_emotion_and_animation(content)
                        print(f"[DEBUG] extract_emotion_and_animation: emotion={emotion}, animation={animation}, content={content!r}")
                        global last_emotion, last_animation, emotion_reset_time, animation_reset_time
                        # –≠–º–æ—Ü–∏—è
                        if emotion:
                            print(f"[DEBUG] –¢—Ä–∏–≥–≥–µ—Ä —ç–º–æ—Ü–∏–∏: {emotion}")
                            if emotion != last_emotion or now > emotion_reset_time:
                                await send_to_overlay(f"emotion:{emotion}")
                                await send_emotion_to_vrm(emotion, 1.0, 2000)
                                last_emotion = emotion
                                emotion_reset_time = now + 2  # 2 —Å–µ–∫—É–Ω–¥—ã (–∏–ª–∏ —Å–∫–æ–ª—å–∫–æ –¥–ª–∏—Ç—Å—è —ç–º–æ—Ü–∏—è)
                        # –î–≤–∏–∂–µ–Ω–∏–µ
                        if animation:
                            print(f"[DEBUG] –¢—Ä–∏–≥–≥–µ—Ä –¥–≤–∏–∂–µ–Ω–∏—è: {animation}")
                            if animation != last_animation or now > animation_reset_time:
                                await send_animation_to_vrm(animation, 2000)
                                last_animation = animation
                                animation_reset_time = now + 2  # 2 —Å–µ–∫—É–Ω–¥—ã (–∏–ª–∏ —Å–∫–æ–ª—å–∫–æ –¥–ª–∏—Ç—Å—è –¥–≤–∏–∂–µ–Ω–∏–µ)
                        # --- –∫–æ–Ω–µ—Ü ---

                        # --- –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ overlay ---
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        # –∏–ª–∏ –∫–æ–≥–¥–∞ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                        text_overlay_config = get('TEXT_OVERLAY', {})
                        min_chars = text_overlay_config.get('update_min_chars', 50)
                        update_on_punct = text_overlay_config.get('update_on_punctuation', True)
                        
                        should_update = len(full_response) - len(last_text_update) > min_chars
                        if update_on_punct:
                            should_update = should_update or any(p in content for p in '.!?')
                        
                        if should_update:
                            await send_text_to_overlay(full_response.strip())
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ VRM
                            await send_text_to_vrm(full_response.strip(), 5000)
                            last_text_update = full_response
                        # --- –∫–æ–Ω–µ—Ü ---

                        while True:
                            match = punctuation_pattern.search(buffer)
                            if not match:
                                break
                            chunk = buffer[:match.end()].strip()
                            buffer = buffer[match.end():]
                            if chunk and speak:
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "talking" –≤ VRM
                                await send_status_to_vrm("talking")
                                await send_speech_to_vrm(True, chunk)
                                
                                await send_to_overlay("start")
                                clean_chunk = clean_text_for_tts(chunk)
                                if clean_chunk.strip():
                                    await speak_text(clean_chunk)
                                await send_to_overlay("stop")
                                
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "idle" –≤ VRM
                                await send_status_to_vrm("idle")
                                await send_speech_to_vrm(False)

                    if json_data.get("done", False):
                        if buffer.strip() and speak:
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "talking" –≤ VRM –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫—É—Å–∫–∞
                            await send_status_to_vrm("talking")
                            await send_speech_to_vrm(True, buffer.strip())
                            
                            clean_buffer = clean_text_for_tts(buffer.strip())
                            if clean_buffer:
                                await speak_text(clean_buffer)
                            
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "idle" –≤ VRM
                            await send_status_to_vrm("idle")
                            await send_speech_to_vrm(False)

                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –ø–∞–º—è—Ç—å
                        memory.add("assistant", full_response.strip())
                        # ‚è≥ –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å (–µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å)
                        memory.save_long_term()
                        steps.append({"name": "llm_response", "duration": time.time() - step_start}); step_start = time.time()
                        break

                except Exception as e:
                    print(f"\n[–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞]: {e}")
                    error_log.append({'error': f'[LLM Parse Error]: {e}', 'timestamp': time.strftime('%H:%M:%S')})

    except requests.RequestException as e:
        print(f"[–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama]: {e}")
        error_log.append({'error': f'[Ollama Error]: {e}', 'timestamp': time.strftime('%H:%M:%S')})
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "idle" –≤ VRM –ø—Ä–∏ –æ—à–∏–±–∫–µ
        await send_status_to_vrm("idle")

    await send_text_to_overlay(full_response.strip())
    steps.append({"name": "overlay_final_text", "duration": time.time() - step_start}); step_start = time.time()
    frame = inspect.currentframe()
    if frame is not None and frame.f_back is not None:
        caller = frame.f_back.f_code.co_name
        if caller not in ['process_message_from_chat']:
            last_answers.append({'source': 'local', 'user': 'user', 'text': full_response.strip(), 'timestamp': time.strftime('%H:%M:%S')})

    llm_cache[cache_key] = full_response.strip()
    steps.append({"name": "cache_and_finish", "duration": time.time() - step_start}); step_start = time.time()
    monitor.timing_history.append({"steps": steps, "total": sum(s["duration"] for s in steps)})
    if len(monitor.timing_history) > 5:
        monitor.timing_history.pop(0)
    return full_response.strip()

async def chat_stream_llamacpp(role_manager, memory, prompt, model_path="E:\\Skai\\llm\\gemma-3n-E4B-it-Q6_K.gguf", speak=True):
    """
    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å llama.cpp –Ω–∞–ø—Ä—è–º—É—é
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫–∞–∫ prompt (–±–µ–∑ —Ä–æ–ª–µ–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
    """
    steps = []
    step_start = time.time()
    cache_key = (role_manager.get_role(), prompt)
    if cache_key in llm_cache:
        full_response = llm_cache[cache_key]
        print("[LLM CACHE] –û—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞.")
        await send_text_to_overlay(full_response.strip())
        last_answers.append({'source': 'local', 'user': 'user', 'text': full_response.strip(), 'timestamp': time.strftime('%H:%M:%S')})
        return full_response.strip()

    memory.add("user", prompt)
    steps.append({"name": "add_to_short_term", "duration": time.time() - step_start}); step_start = time.time()

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ –ø–∞–º—è—Ç–∏ ---
    memory_response = memory.handle_memory_commands(prompt)
    steps.append({"name": "memory_commands", "duration": time.time() - step_start}); step_start = time.time()
    if memory_response:
        print(f"[MEMORY] {memory_response}")
        await send_text_to_overlay(memory_response)
        last_answers.append({'source': 'local', 'user': 'user', 'text': memory_response, 'timestamp': time.strftime('%H:%M:%S')})
        monitor.timing_history.append({"steps": steps, "total": sum(s["duration"] for s in steps)})
        if len(monitor.timing_history) > 5:
            monitor.timing_history.pop(0)
        return memory_response

    # --- llama.cpp: prompt —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ---
    full_prompt = prompt

    buffer = ""
    full_response = ""
    last_text_update = ""

    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å "thinking" –≤ VRM
        await send_status_to_vrm("thinking")
        steps.append({"name": "overlay_thinking", "duration": time.time() - step_start}); step_start = time.time()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º llama.cpp –∫–∞–∫ –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å
        import subprocess
        import json
        
        # –ü—É—Ç—å –∫ llama-cli.exe
        llama_path = "E:\\Skai\\llama_vulkan\\build\\bin\\Release\\llama-cli.exe"
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞
        cmd = [
            llama_path,
            "-m", model_path,
            "-p", full_prompt,
            "-n", "200",  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            "-t", "8",    # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
            "--no-warmup"  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–æ–≥—Ä–µ–≤
        ]
        
        print(f"[LLAMACPP] –ó–∞–ø—É—Å–∫: {' '.join(cmd)}")
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        
        steps.append({"name": "llm_request_start", "duration": time.time() - step_start}); step_start = time.time()
        
        # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ —Å—Ä–∞–∑—É (llama.cpp –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥)
        stdout, stderr = process.communicate()
        
        if stderr:
            print(f"[LLAMACPP STDERR]: {stderr}")
        
        if stdout:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–≤–æ–¥ –∫–∞–∫ –µ–¥–∏–Ω—ã–π –±–ª–æ–∫
            output = stdout.strip()
            print(output, end="", flush=True)
            full_response = output
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç —Å—Ä–∞–∑—É
            if full_response.strip() and speak:
                await send_status_to_vrm("talking")
                await send_speech_to_vrm(True, full_response.strip())
                
                clean_response = clean_text_for_tts(full_response.strip())
                if clean_response:
                    await speak_text(clean_response)
                
                await send_status_to_vrm("idle")
                await send_speech_to_vrm(False)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –ø–∞–º—è—Ç—å
        memory.add("assistant", full_response.strip())
        memory.save_long_term()
        steps.append({"name": "llm_response", "duration": time.time() - step_start}); step_start = time.time()

    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞ llama.cpp]: {e}")
        error_log.append({'error': f'[LLAMACPP Error]: {e}', 'timestamp': time.strftime('%H:%M:%S')})
        await send_status_to_vrm("idle")

    await send_text_to_overlay(full_response.strip())
    steps.append({"name": "overlay_final_text", "duration": time.time() - step_start}); step_start = time.time()
    
    frame = inspect.currentframe()
    if frame is not None and frame.f_back is not None:
        caller = frame.f_back.f_code.co_name
        if caller not in ['process_message_from_chat']:
            last_answers.append({'source': 'local', 'user': 'user', 'text': full_response.strip(), 'timestamp': time.strftime('%H:%M:%S')})

    llm_cache[cache_key] = full_response.strip()
    steps.append({"name": "cache_and_finish", "duration": time.time() - step_start}); step_start = time.time()
    monitor.timing_history.append({"steps": steps, "total": sum(s["duration"] for s in steps)})
    if len(monitor.timing_history) > 5:
        monitor.timing_history.pop(0)
    return full_response.strip()

async def tts_to_file(text: str, filename: Optional[str] = None) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–∑–≤—É—á–∫—É —Ç–µ–∫—Å—Ç–∞ –≤ mp3- –∏–ª–∏ wav-—Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
    –ï—Å–ª–∏ filename –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî —Å–æ–∑–¥–∞—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –≤ TMP_FOLDER.
    –§–∞–π–ª –Ω–µ —É–¥–∞–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!
    """
    tts_engine = get_tts_engine()
    ext = '.wav' if tts_engine == 'silero' else '.mp3'
    if not filename:
        filename = os.path.join(TMP_FOLDER, f"{uuid.uuid4().hex}{ext}")
    if filename is None:
        raise ValueError("filename –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å None")
    if tts_engine == 'silero':
        silero_tts(text, filename)
    else:
        communicate = edge_tts.Communicate(text, VOICE)
        await communicate.save(filename)
    return filename

def periodic_temp_audio_cleanup(folder=TMP_FOLDER, max_age_sec=3600, interval_sec=1800):
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã —Å—Ç–∞—Ä—à–µ max_age_sec –∏–∑ –ø–∞–ø–∫–∏ folder.
    """
    def cleanup():
        while True:
            now = time.time()
            for fname in os.listdir(folder):
                fpath = os.path.join(folder, fname)
                try:
                    if os.path.isfile(fpath):
                        if now - os.path.getmtime(fpath) > max_age_sec:
                            os.remove(fpath)
                except Exception as e:
                    print(f"[TEMP_AUDIO CLEANUP ERROR]: {e}")
            time.sleep(interval_sec)
    t = threading.Thread(target=cleanup, daemon=True)
    t.start()

# –ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
periodic_temp_audio_cleanup()