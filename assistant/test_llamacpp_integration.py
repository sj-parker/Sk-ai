#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ llama.cpp —Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º
"""

import asyncio
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import get
from memory.user_context import get_user_context
from audio_output import chat_stream_llamacpp

async def test_llamacpp():
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ llama.cpp...")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    memory, role_manager = get_user_context("local", 'local')
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ llama.cpp
    llm_config = get('LLM', {}).get('llamacpp', {})
    model_path = llm_config.get('model_path', 'E:\\Skai\\llm\\gemma-3n-E4B-it-Q6_K.gguf')
    
    print(f"üìÅ –ú–æ–¥–µ–ª—å: {model_path}")
    print(f"üîß –ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª: {llm_config.get('executable_path')}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
    test_prompt = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
    
    print(f"\nüí¨ –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç: {test_prompt}")
    print("‚è≥ –û–∂–∏–¥–∞–µ–º –æ—Ç–≤–µ—Ç...")
    
    try:
        # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é llama.cpp
        response = await chat_stream_llamacpp(
            role_manager=role_manager,
            memory=memory,
            prompt=test_prompt,
            model_path=model_path,
            speak=False  # –û—Ç–∫–ª—é—á–∞–µ–º TTS –¥–ª—è —Ç–µ—Å—Ç–∞
        )
        
        print(f"\n‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω:")
        print(f"üìù {response}")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_llamacpp()) 